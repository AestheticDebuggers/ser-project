{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cd87e9-c8bb-485b-b928-eea1f294bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Data Augmentation: Add White Noise\n",
    "def augment_data(data):\n",
    "    ratio = 0.005\n",
    "    max_val = np.amax(data)\n",
    "    r_uniform = np.random.uniform()\n",
    "    noise_factor = ratio * max_val * r_uniform\n",
    "    noise = np.random.randn(len(data))\n",
    "    augmented_data = data + noise_factor * noise\n",
    "    return augmented_data\n",
    "\n",
    "# Log-Mel Spectrogram Extraction\n",
    "def extract_log_mel_spectrogram(audio, sr=22050, n_mels=128):\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    return log_mel_spectrogram\n",
    "\n",
    "\n",
    "# CNN+LSTM Model\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        \n",
    "        # LFLB Block 1\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.elu1 = nn.ELU()\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        # LFLB Block 2\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.elu2 = nn.ELU()\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        # LFLB Block 3\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.elu3 = nn.ELU()\n",
    "        self.max_pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        # LFLB Block 4\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.elu4 = nn.ELU()\n",
    "        self.max_pool4 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout4 = nn.Dropout(0.3)\n",
    "        \n",
    "        # LSTM and FC Layers\n",
    "        self.lstm = nn.LSTM(input_size=512, hidden_size=128, num_layers=2, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(128, 8)  # 8 classes for emotions\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.elu1(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.elu2(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.elu3(x)\n",
    "        x = self.max_pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.elu4(x)\n",
    "        x = self.max_pool4(x)\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        x = x.permute(0, 2, 3, 1)  # Reorder dimensions for LSTM\n",
    "        x = x.reshape(x.size(0), x.size(1), -1)  # Flatten to 3D for LSTM\n",
    "        x = x[:, :, :512]  # Reshape to match LSTM input size\n",
    "        \n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])  # Use the last output from LSTM\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Function to load RAVDESS dataset from subdirectories\n",
    "def load_ravdess_data(data_dir, n_mels=128, augment=False):\n",
    "    audio_files = []\n",
    "    labels = []\n",
    "\n",
    "    for actor_path in os.listdir(data_dir):\n",
    "        for file in os.listdir(os.path.join(data_dir, actor_path)):\n",
    "            if file.endswith('.wav'):\n",
    "                audio, sr = librosa.load(os.path.join(data_dir, actor_path, file))\n",
    "                if augment:\n",
    "                    audio = augment_data(audio, sr=sr)\n",
    "                \n",
    "                # Extract mel spectrogram\n",
    "                S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels)\n",
    "                S_DB = librosa.power_to_db(S, ref=np.max)\n",
    "                \n",
    "                # Pad the spectrogram to a fixed length\n",
    "                max_length = 1440  # adjust this value according to your needs\n",
    "                if S_DB.shape[1] > max_length:\n",
    "                    S_DB = S_DB[:, :max_length]\n",
    "                else:\n",
    "                    padding_width = max_length - S_DB.shape[1]\n",
    "                    S_DB = np.pad(S_DB, ((0, 0), (0, padding_width)), mode='constant')\n",
    "                \n",
    "                audio_files.append(S_DB)\n",
    "                \n",
    "                # Extract label from filename\n",
    "                try:\n",
    "                    label = int(file.split('-')[2]) - 1  # Adjust index if necessary\n",
    "                    labels.append(label)\n",
    "                except IndexError:\n",
    "                    print(f\"Issue with file: {file}. Check filename structure.\")\n",
    "                    continue\n",
    "\n",
    "    print(\"Loaded RAVDESS audio files:\", len(audio_files))\n",
    "    print(\"Loaded RAVDESS labels:\", len(labels))\n",
    "\n",
    "    return audio_files, labels\n",
    "\n",
    "\n",
    "def load_savee_data(data_dir, n_mels=128, augment=False):\n",
    "    audio_files = []\n",
    "    labels = []\n",
    "\n",
    "    emotion_dict = {\n",
    "        'a': 0,  # 'angry'\n",
    "        'd': 1,  # 'disgust'\n",
    "        'f': 2,  # 'fear'\n",
    "        'h': 3,  # 'happy'\n",
    "        'n': 4,  # 'neutral'\n",
    "        'sa': 5,  # 'sad'\n",
    "        'su': 6  # 'surprise'\n",
    "    }\n",
    "\n",
    "    all_files_path = os.path.join(data_dir, 'ALL')\n",
    "    for file in os.listdir(all_files_path):\n",
    "        if file.endswith('.wav'):\n",
    "            emotion_key = file.split('_')[1][0]  # Extract the first character of the emotion code\n",
    "            if emotion_key in emotion_dict:\n",
    "                emotion_label = emotion_dict[emotion_key]\n",
    "                audio, sr = librosa.load(os.path.join(all_files_path, file))\n",
    "                if augment:\n",
    "                    audio = augment_data(audio, sr=sr)\n",
    "\n",
    "                # Extract mel spectrogram\n",
    "                S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels)\n",
    "                S_DB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "                # Pad the spectrogram to a fixed length\n",
    "                max_length = 1440  # adjust this value according to your needs\n",
    "                if S_DB.shape[1] > max_length:\n",
    "                    S_DB = S_DB[:, :max_length]\n",
    "                else:\n",
    "                    padding_width = max_length - S_DB.shape[1]\n",
    "                    S_DB = np.pad(S_DB, ((0, 0), (0, padding_width)), mode='constant')\n",
    "\n",
    "                audio_files.append(S_DB)\n",
    "                labels.append(emotion_label)\n",
    "\n",
    "    print(\"Loaded SAVEE audio files:\", len(audio_files))\n",
    "    print(\"Loaded SAVEE labels:\", len(labels))\n",
    "\n",
    "    return audio_files, labels\n",
    "\n",
    "\n",
    "def load_data(data_dir, dataset=\"RAVDESS\", n_mels=128, augment=False):\n",
    "    if dataset == \"RAVDESS\":\n",
    "        return load_ravdess_data(data_dir, n_mels=n_mels, augment=augment)\n",
    "    elif dataset == \"SAVEE\":\n",
    "        return load_savee_data(data_dir, n_mels=n_mels, augment=augment)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset. Please choose either 'RAVDESS' or 'SAVEE'.\")\n",
    "\n",
    "\n",
    "# Custom Dataset Class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        X_tensor = torch.FloatTensor(X).unsqueeze(0)  # Add channel dimension\n",
    "        y_tensor = torch.LongTensor([y])\n",
    "        return X_tensor, y_tensor\n",
    "\n",
    "# Split data into train and validation sets\n",
    "def split_data(X, y, test_size=0.2):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "# Train Model Function\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=30, learning_rate=0.001, label_smoothing=0.1):\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, min_lr=1e-5)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch', ncols=80):\n",
    "            inputs, labels = inputs.to(device), labels.to(device).squeeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_acc = correct_train / total_train\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device).squeeze(1)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = correct_val / total_val\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        scheduler.step(val_loss)  # Adjust learning rate based on validation loss\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "\n",
    "# SFS-guided WOA Optimization Function\n",
    "def sfs_guided_woa(maxiter, n, w1_range=(0, 0.5), w2_range=(0, 1), w3_range=(0, 1)):\n",
    "    # Initialize population\n",
    "    population = np.random.random((n, 2))  # Randomly initialize population of solutions\n",
    "    fitness = np.zeros(n)  # Array to store fitness values\n",
    "\n",
    "    # WOA parameters\n",
    "    a = 0.5\n",
    "    A = 2\n",
    "    C = 2 * np.random.random() - 1\n",
    "    l = np.random.random()\n",
    "\n",
    "    # Guided-WOA parameters\n",
    "    w1 = np.random.uniform(*w1_range)\n",
    "    w2 = np.random.uniform(*w2_range)\n",
    "    w3 = np.random.uniform(*w3_range)\n",
    "\n",
    "    # Initialization of best solution\n",
    "    best_solution = population[np.argmax(fitness)]\n",
    "\n",
    "    for t in range(maxiter):\n",
    "        z = 1 - (t / maxiter) ** 2\n",
    "\n",
    "        for i in range(n):\n",
    "            r1, r2, r3 = np.random.random(3)\n",
    "\n",
    "            if r3 < 0.5:\n",
    "                if np.abs(A) < 1:\n",
    "                    D = np.abs(C * best_solution - population[i])\n",
    "                    population[i] = best_solution - A * D\n",
    "                else:\n",
    "                    # Select three random solutions\n",
    "                    rand_indices = np.random.choice(np.arange(n), size=3, replace=False)\n",
    "                    Wrand1, Wrand2, Wrand3 = population[rand_indices]\n",
    "\n",
    "                    population[i] = w1 * Wrand1 + z * w2 * (Wrand2 - Wrand3) + (1 - z) * w3 * (best_solution - Wrand1)\n",
    "            else:\n",
    "                b = 1\n",
    "                D = np.exp(b * l) * np.cos(2 * np.pi * l)\n",
    "                population[i] = D * np.exp(a * l) * np.cos(2 * np.pi * l) + best_solution\n",
    "\n",
    "            # Clip solutions to keep them within valid ranges (if needed)\n",
    "            population[i] = np.clip(population[i], 0, 1)\n",
    "\n",
    "            # Apply sigmoid function to convert to range [0, 1]\n",
    "            population[i] = 1 / (1 + np.exp(-population[i]))\n",
    "\n",
    "            # Evaluate fitness (dummy fitness function)\n",
    "            fitness[i] = np.sum(population[i])  # Replace with actual fitness evaluation\n",
    "\n",
    "        # Update best_solution\n",
    "        if np.max(fitness) > np.max(np.array([np.sum(best_solution)])):\n",
    "            best_solution = population[np.argmax(fitness)]\n",
    "\n",
    "        # Update WOA parameters (randomize for diversity)\n",
    "        A = 2 * np.random.random() - 1\n",
    "        C = 2 * np.random.random() - 1\n",
    "        l = np.random.random()\n",
    "        w1 = np.random.uniform(*w1_range)\n",
    "        w2 = np.random.uniform(*w2_range)\n",
    "        w3 = np.random.uniform(*w3_range)\n",
    "\n",
    "    return best_solution\n",
    "\n",
    "# Main function to load data, create dataset, define model, and perform optimization\n",
    "def main():\n",
    "    # Parameters\n",
    "    data_dir_ravdess = \"C:\\\\Users\\\\User\\\\Downloads\\\\ravdess\"\n",
    "    data_dir_savee = \"C:\\\\Users\\\\User\\\\Downloads\\\\savee\"\n",
    "    num_epochs = 5\n",
    "    maxiter = 100  # Maximum number of iterations for SFS-guided WOA\n",
    "    n = 10  # Population size\n",
    "\n",
    "    # Load RAVDESS data\n",
    "    X_ravdess, y_ravdess = load_data(data_dir_ravdess, dataset='RAVDESS')\n",
    "    print()\n",
    "\n",
    "    # Load SAVEE data\n",
    "    X_savee, y_savee = load_data(data_dir_savee, dataset='SAVEE')\n",
    "    print()\n",
    "\n",
    "    # Combine data and labels\n",
    "    X = np.concatenate([X_ravdess, X_savee], axis=0)\n",
    "    y = np.concatenate([y_ravdess, y_savee], axis=0)\n",
    "\n",
    "    print(f\"Combined Data - Total Samples: {len(X)}\")\n",
    "    print(f\"RAVDESS Data Samples: {len(X_ravdess)}\")\n",
    "    print(f\"SAVEE Data Samples: {len(X_savee)}\")\n",
    "\n",
    "    # Split data into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = split_data(X, y, test_size=0.2)\n",
    "\n",
    "    # Create DataLoader for training and validation sets\n",
    "    train_dataset = AudioDataset(X_train, y_train)\n",
    "    val_dataset = AudioDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize and train the model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CNNLSTM().to(device)\n",
    "\n",
    "    # Train the model using the default hyperparameters\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = train_model(model, train_loader, val_loader, device,\n",
    "                                                                             num_epochs=num_epochs,\n",
    "                                                                             learning_rate=0.001,\n",
    "                                                                             label_smoothing=0.1)\n",
    "\n",
    "    # Plotting results (optional)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Losses')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Accuracies')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cbf186-bf6e-451f-afd9-12ef5d359dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNNLSTM().to(device)\n",
    "\n",
    "# Example usage of inference function\n",
    "def infer_emotion(model, audio_file):\n",
    "    # Load audio and extract log mel spectrogram\n",
    "    audio, sr = librosa.load(audio_file)\n",
    "    log_mel_spectrogram = extract_log_mel_spectrogram(audio, sr=sr)\n",
    "    \n",
    "    # Normalize the spectrogram data (similar to training data preprocessing)\n",
    "    scaler = StandardScaler()\n",
    "    log_mel_spectrogram_scaled = scaler.fit_transform(log_mel_spectrogram)\n",
    "    \n",
    "    # Convert to PyTorch tensor and add batch dimension\n",
    "    spectrogram_tensor = torch.tensor(log_mel_spectrogram_scaled).unsqueeze(0).unsqueeze(0).float()\n",
    "    \n",
    "    # Move tensor to device (GPU if available)\n",
    "    device = next(model.parameters()).device\n",
    "    spectrogram_tensor = spectrogram_tensor.to(device)\n",
    "    \n",
    "    # Perform inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(spectrogram_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Return predicted emotion index\n",
    "    return predicted.item()\n",
    "\n",
    "\n",
    "# Example usage of plotting functions and inference\n",
    "audio_file = 'C:\\\\Users\\\\User\\\\Downloads\\\\ravdess\\\\Actor_13\\\\03-01-08-01-02-02-13.wav'\n",
    "emotion = infer_emotion(model, audio_file)\n",
    "emotion_labels = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
    "predicted_emotion_label = emotion_labels[emotion]\n",
    "print(f'Predicted Emotion: {predicted_emotion_label}')\n",
    "\n",
    "# Plot mel spectrogram\n",
    "def plot_mel_spectrogram(audio_file):\n",
    "    audio, sr = librosa.load(audio_file)\n",
    "    log_mel_spectrogram = extract_log_mel_spectrogram(audio, sr=sr)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(log_mel_spectrogram, sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Mel spectrogram')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_mel_spectrogram(audio_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
